Download Here ---> https://tinyurl.com/ycx3x5fn?id=2162445



World of Yo-Ho


ÐœÐ¸Ñ€ Ð™Ð¾-Ð¥Ð¾ Ñ†Ð¸Ñ„Ñ€Ð¾Ð²Ð¾Ð¹ ÐºÐ¾Ð¼Ð¿Ð°Ð½ÑŒÐ¾Ð½ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¾Ð±Ð¾Ð¹ Ð±ÐµÑÐ¿Ð»Ð°Ñ‚Ð½Ñ‹Ð¹ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ Ð¿Ñ€ÐµÐ´Ð½Ð°Ð·Ð½Ð°Ñ‡ÐµÐ½ Ð´Ð»Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ Ð¼Ð¸Ñ€Ð¾Ð¼ Ð™Ð¾-Ð¥Ð¾ Ð½Ð°ÑÑ‚Ð¾Ð»ÑŒÐ½Ð¾Ð¹ Ð¸Ð³Ñ€Ñ‹.


ÐœÐ¸Ñ€ Yo-Ð¥Ð¾ ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð¸Ð³Ñ€Ð° Ñ„Ð°Ð½Ñ‚Ð°Ð·Ð¸Ð¸ Ð¿Ñ€Ð¸ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ð¹ Ð¸ Ð¿Ð¸Ñ€Ð°Ñ‚ÑÑ‚Ð²Ð° Ð² Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¾Ð¼ Ð¼Ð¾Ñ€Ðµ. Ð­Ñ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ‚Ð¸Ð¿
Ð¸Ð³Ñ€Ñ‹, Ð¾Ñ‚ 2 Ð´Ð¾ 4 Ð¸Ð³Ñ€Ð¾ÐºÐ¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑÐ¾Ñ‡ÐµÑ‚Ð°ÐµÑ‚ Ð² ÑÐµÐ±Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¸ ÑÐ¾Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾Ðµ ÑƒÐ´Ð¾Ð²Ð¾Ð»ÑŒÑÑ‚Ð²Ð¸Ðµ Ð¾Ñ‚ Ð¸Ð³Ñ€Ð¾Ð²Ð¾Ð³Ð¾ Ñ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¼Ð¸ Ð¼ÐµÑ…Ð°Ð½Ð¸ÐºÐ¸ Ð²Ð¸Ð´ÐµÐ¾ Ð¸Ð³Ñ€Ñ‹. Ð¡Ñ‚Ð°Ñ‚ÑŒ ÑÐ°Ð¼Ñ‹Ð¼ Ð¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¼ Ð¿Ð¸Ñ€Ð°Ñ‚Ð¾Ð¼ ÑÑ‚Ð¾Ð¼ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð¼ Ð¼Ð¸Ñ€Ðµ, Ð¿Ð¾Ð»Ð½Ð¾Ð¼ ÑƒÐ¼Ð½Ñ‹Ñ… Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ñ‹Ñ…, Ð¿Ð¾Ð³Ð¸Ð±ÑˆÐ¸Ñ… Ð¾ÑÑ‚Ñ€Ð¾Ð²Ð¾Ð² Ð¸ Ð´Ð¸ÐºÐ¾Ð¹ Ð¼Ð°Ð³Ð¸Ð¸.


â€¢ Ð’Ð°Ñˆ Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½ Ð’Ð°Ñˆ ÐºÐ¾Ñ€Ð°Ð±Ð»ÑŒ! ÐžÐ½ Ð·Ð½Ð°ÐµÑ‚, Ð³Ð´Ðµ Ð¾Ð½ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑÑ Ð½Ð° ÐºÐ°Ñ€Ñ‚Ðµ, Ð¾Ð½Ð° Ð´Ð²Ð¸Ð¶ÐµÑ‚ÑÑ Ð²Ð¾ÐºÑ€ÑƒÐ³ Ð´Ð¾ÑÐºÐ¸ Ð¾Ð½ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²ÑƒÐµÑ‚ Ñ Ð´Ñ€ÑƒÐ³Ð¸Ð¼Ð¸ Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½Ð°Ð¼Ð¸!


â€¢ Ð¿ÐµÑ€ÐµÑ…Ð¸Ñ‚Ñ€Ð¸Ñ‚ÑŒ Ð²Ð°ÑˆÐ¸Ñ… ÐºÐ¾Ð»Ð»ÐµÐ³ Ð¿Ð¸Ñ€Ð°Ñ‚Ð¾Ð². Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ñ‚ÑŒ ÑÐ²Ð¾Ð¸ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ñ‚Ð°Ð¹Ð½Ð¾ Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ð²Ð°Ñˆ Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½, Ð° Ð·Ð°Ñ‚ÐµÐ¼ Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÑŒ ÐµÐ³Ð¾ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð½Ð° ÐºÐ°Ñ€Ñ‚Ñƒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ð´ÐµÐ»Ð¸Ñ‚ÑŒÑÑ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸ Ñ Ð´Ñ€ÑƒÐ·ÑŒÑÐ¼Ð¸ Ð¸ / Ð¸Ð»Ð¸ Ð²Ñ€Ð°Ð³Ð°Ð¼Ð¸!


â€¢ ÐŸÐ¾ÑˆÐ°Ð³Ð¾Ð²Ð°Ñ Ð¸Ð³Ñ€Ð° Ð² Ð¶Ð¸Ð·Ð½Ð¸, Ð°Ð½Ð¸Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¼Ð¸Ñ€, Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹, Ð¿Ð¾Ð²Ð¾Ñ€Ð¾Ñ‚Ð¾Ð² Ð¸ Ð¼Ð¾Ð½ÑÑ‚Ñ€Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ÑÑ, ÐºÐ°Ðº Ð²Ñ‹ Ð¸Ð³Ñ€Ð°ÐµÑ‚Ðµ.


â€¢ Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚Ðµ Ð²Ð°ÑˆÑƒ Ð¸Ð³Ñ€Ñƒ, Ð¸ Ð²Ñ‹ Ð¸ Ð²Ð°ÑˆÐ¸ Ð´Ñ€ÑƒÐ·ÑŒÑ Ð¼Ð¾Ð³ÑƒÑ‚ Ð²Ð¾Ð·Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ÐµÐ³Ð¾, ÐºÐ¾Ð³Ð´Ð° Ð²Ñ‹ Ñ…Ð¾Ñ‚Ð¸Ñ‚Ðµ!


HpWang-whu/YOHO


This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.


Name already in use


A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?


Sign In Required


Please sign in to use Codespaces.


Launching GitHub Desktop


If nothing happens, download GitHub Desktop and try again.


Launching GitHub Desktop


If nothing happens, download GitHub Desktop and try again.


Launching Xcode


If nothing happens, download Xcode and try again.


Launching Visual Studio Code


Your codespace will open once ready.


There was a problem preparing your codespace, please try again.


Latest commit


Git stats


Files


Failed to load latest commit information.


README.md 


You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors


In this paper, we propose a novel local descriptor-based framework, called You Only Hypothesize Once (YOHO), for the registration of two unaligned point clouds. In contrast to most existing local descriptors which rely on a fragile local reference frame to gain rotation invariance, the proposed descriptor achieves the rotation invariance by recent technologies of group equivariant feature learning, which brings more robustness to point density and noise. Meanwhile, the descriptor in YOHO also has a rotation equivariant part, which enables us to estimate the registration from just one correspondence hypothesis. Such property reduces the searching space for feasible transformations, thus greatly improves both the accuracy and the efficiency of YOHO. Extensive experiments show that YOHO achieves superior performances with much fewer needed RANSAC iterations on four widely-used datasets, the 3DMatch/3DLoMatch datasets, the ETH dataset and the WHU-TLS dataset.



  
2023-02-05: YOHO has been extended to IEEE TPAMI 2023! ðŸŽ‰ ðŸŽ‰ [RoReg]

  
2022-06-30: YOHO is accepted by ACM MM 2022!

  
2021-09-01: The Preprint Paper is accessible on arXiv.

  
2021-07-06: YOHO using FCGF backbone is released.




Here we offer the FCGF backbone YOHO. Thus FCGF requirements need to be met:



  
Ubuntu 14.04 or higher

  
CUDA 11.1 or higher

  
Python v3.7 or higher

  
Pytorch v1.6 or higher

  
MinkowskiEngine v0.5 or higher




Specifically, The code has been tested with:


conda create -n fcgf_yoho python=3.7 conda activate fcgf_yoho 


conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=11.0 -c pytorch 


cd MinkowskiEngine conda install openblas-devel -c anaconda export CUDA_HOME=/usr/local/cuda-11.1 #We have checked cuda-11.1. python setup.py install --blas_include_dirs=$/include --blas=openblas cd .. 


pip install git+https://github.com/NVIDIA/MinkowskiEngine.git 


pip install -r requirements.txt 


cd knn_search/ export CUDA_HOME=/usr/local/cuda-11.1 #We have checked cuda-11.1. python setup.py build_ext --inplace cd .. 


Dataset & Pretrained model


The datasets and pretrained weights have been uploaded to Google Cloud:


Also, all data above can be downloaded in BaiduDisk(Code:0di4).


Datasets above contain the point clouds (.ply) and keypoints (.txt, 5000 per point cloud) files. Please place the data to ./data/origin_data following the example data structure as:


data/ â”œâ”€â”€ origin_data/ â”œâ”€â”€ 3dmatch/ â””â”€â”€ kitchen/ â”œâ”€â”€ PointCloud/ â”œâ”€â”€ cloud_bin_0.ply â”œâ”€â”€ gt.log â””â”€â”€ gt.info â””â”€â”€ Keypoints/ â””â”€â”€ cloud_bin_0Keypoints.txt â”œâ”€â”€ 3dmatch_train/ â”œâ”€â”€ ETH/ â””â”€â”€ WHU-TLS/ 


Pretrained weights we offer include FCGF Backbone, Part I and Part II. Which have been added to the main branch and organized following the structure as:


model/ â”œâ”€â”€ Backbone/ â””â”€â”€ best_bal_checkpoint.pth â”œâ”€â”€ PartI_train/ â””â”€â”€ model_best.pth â””â”€â”€ PartII_train/ â””â”€â”€ model_best.pth 


To train YOHO, the group input of train set should be prepared using the FCGF model we offer, which is trained with rotation argument in [0,50] deg, by command:


Warning: the process above needs 300G storage space.


The training of YOHO is two-stage, you can run which with the commands sequentially:


python Train.py --Part PartI python Train.py --Part PartII 


With the Pretrained/self-trained models, you can try YOHO with:


python YOHO_testset.py --dataset demo python Demo.py 


Test on the 3DMatch and 3DLoMatch


To evalute YOHO on 3DMatch and 3DLoMatch:


python YOHO_testset.py --dataset 3dmatch --voxel_size 0.025 


python Test.py --Part PartI --max_iter 1000 --dataset 3dmatch #YOHO-C on 3DMatch python Test.py --Part PartI --max_iter 1000 --dataset 3dLomatch #YOHO-C on 3DLoMatch python Test.py --Part PartII --max_iter 1000 --dataset 3dmatch #YOHO-O on 3DMatch python Test.py --Part PartII --max_iter 1000 --dataset 3dLomatch #YOHO-O on 3DLoMatch 


Generalize to the ETH dataset


The generalization results on the outdoor ETH dataset can be got as follows:


python YOHO_testset.py --dataset ETH --voxel_size 0.15 



  
Change the parameter batch_size in YOHO_testset.py-->batch_feature_extraction()-->loader from 4 to 1

  
Carry out the command scene by scene by controlling the scene processed now in utils/dataset.py-->get_dataset_name()-->if name==ETH




python Test.py --Part PartI --max_iter 1000 --dataset ETH --ransac_d 0.2 --tau_2 0.2 --tau_3 0.5 #YOHO-C on ETH python Test.py --Part PartII --max_iter 1000 --dataset ETH --ransac_d 0.2 --tau_2 0.2 --tau_3 0.5 #YOHO-O on ETH 


Generalize to the WHU-TLS dataset


The generalization results on the outdoor WHU-TLS dataset can be got as follows:


python YOHO_testset.py --dataset WHU-TLS --voxel_size 0.8 


python Test.py --Part PartI --max_iter 1000 --dataset WHU-TLS --ransac_d 1 --tau_2 0.5 --tau_3 1 #YOHO-C on WHU-TLS python Test.py --Part PartII --max_iter 1000 --dataset WHU-TLS --ransac_d 1 --tau_2 0.5 --tau_3 1 #YOHO-O on WHU-TLS 


Customize YOHO according to your own needs


To test YOHO on other datasets, or to implement YOHO using other backbones according to your own needs, please refer to Here.


Please consider citing YOHO if this program benefits your project


@inproceedings, author=, booktitle=, pages=, year= > 


Welcome to take a look at the homepage of our research group WHU-USI3DV !


We sincerely thank the excellent projects:



  
EMVN for the group details;

  
FCGF for the backbone;

  
3DMatch for the 3DMatch dataset;

  
Predator for the 3DLoMatch dataset;

  
ETH for the ETH dataset;

  
WHU-TLS for the WHU-TLS dataset;

  
PerfectMatch for organizing the 3DMatch and ETH dataset.




About


[ACM MM 2022] You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors
