Download Here ---> https://tinyurl.com/ycx3x5fn?id=5550521



How to Use Google Lens on the iPhone


Craig Lloyd is a smarthome expert with nearly ten years of professional writing experience. His work has been published by iFixit, Lifehacker, Digital Trends, Slashgear, and GottaBeMobile. Read more. 


If you’re a fan of Google Lens, you now have another avenue for accessing the feature in iOS, because it’s now included in the Google Search app on the iPhone. Here’s how to use it. Google Lens is a neat little feature that can identify real-world objects, like signs, buildings, books, plants, and more using your phone’s camera. And it gives you more information about the object. It’s available in the Google Photos app, but if you don’t use Google Photos, you can now access Google Lens in the regular Google Search app. We’ll show you how to use Google Lens in both apps.


In the Google Search App


If you don’t already have the app, you can download it here. Once you’re up and running, just start by tapping on the Google Lens icon inside of the search bar. If you don’t see the icon, try closing out of the app completely and the re-opening it.


With the camera up, simply point it at an object that you want to identify or learn more about and tap on it.


From there, Google Lens will do its best to identify what’s in the frame and give you results based on what it knows.


In the Google Photos App


One big difference between the Google Search app and the Google Photos app as far as Google Lens is concerned, is that you have to snap a photo first, and then open it within Google Photos to launch Google Lens. However, it’s still easy to access. If you don’t already have the Google Photos app, you can download it here. Once you’re up and running, start by selecting a photo in the feed. We’ll select this photo of my cat.


Select “Continue” at the bottom. This will launch Google Lens and will immediately bring up whatever information about the photo that Google can find.


Craig Lloyd 
Craig Lloyd is a smarthome expert with nearly ten years of professional writing experience. His work has been published by iFixit, Lifehacker, Digital Trends, Slashgear, and GottaBeMobile. 
Read Full Bio »


Your iPhone will soon get Apple's answer to Google Lens


Google Lens has slowly become one of the most useful augmented reality apps around, so Apple has decided build its own rival into iOS 15.


At WWDC 2021, Apple announced that 'Live Text' and 'Visual Look Up' will be coming to the iPhone's camera and Photos app as part of iOS 15. And both are direct rivals to Google Lens, which has become an increasingly powerful way to search the real world through your smartphone camera on both Android and iOS.


While we've already seen something similar on Android phones, 'Live Text' looks like it'll be a handy way for iPhone users turn copy handwritten or printed text from the real world into digital text. Apple says it's based on 'deep neural networks' that use on-device processing, rather than a cloud-based approach.


The example Apple showed was notes on an office whiteboard – you'll be able to tap a new icon in the bottom-right corner of the Camera app's viewfinder, then just use Apple's usual text selection gestures (dragging your finger over the text) to copy the handwritten text into an email or the Notes app.


You'll also be to use 'Live Text' on existing photos in your library, although these use cases look slightly less useful. Apple's examples were copying the name and phone number of a restaurant in the background of an old photo, but perhaps some more interesting uses for the tech will materialize when it's out in the real world.


Apple's 'Live Text' is naturally a lot more limited than Google Lens, given the latter has been out since 2017. Right now, Apple says it only understands seven languages (English, Chinese, French, Italian, German, Spanish, Portuguese), which pails in comparison by Google Lens' ability to translate words into over 100 languages.


This means one of Google Lens' more useful tricks – live translating restaurant menus or signs when you're traveling – won't quite be matched by Apple's 'Live Text' right now. But it's a useful new trick for Apple's Camera app all the same and works across all types of photos, including screenshots and photos on the web.


Searching the real world


In a similar vein, Apple's new 'Visual Look Up' is another direct challenge to some of the main features of Google Lens.


While it wasn't shown in great depth during WWDC 2021, the feature will apparently let you automatically look up information in your photos, like the breed of a dog or the type of flower you snapped.


According to Apple, this will work for books, art, nature, pets and landmarks, although exactly how exhaustive its knowledge is remains to be seen. It'll certainly be tricky for it to compete with Google on this front, given the mountains of data the search giant is able to glean from its other services.


But while this feature will probably be a little limited initially, it seems likely that this move is related to augmented reality and, perhaps ultimately, the rumored Apple Glasses. Automatically identifying visual information, like landmarks, is likely to be an important component of any smart glasses, so 'Visual Look Up' could be considered another important step towards some Apple face furniture.


'Visual Look Up' will apparently also work across iPhone, iPad and Mac, so it'll be baked into whatever Apple device you're using, as long as it's updated to the latest software. You can expect the full release of iOS 15 to arrive in around mid-September.


TechRadar Newsletter


Sign up to receive daily breaking news, reviews, opinion, analysis, deals and more from the world of tech.


By submitting your information you agree to the Terms & Conditions (opens in new tab) and Privacy Policy (opens in new tab) and are aged 16 or over.
